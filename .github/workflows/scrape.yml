name: RSS Scraper - Hourly Execution

on:
  schedule:
    # Run at the top of every hour (0 minutes past the hour)
    - cron: '0 * * * *'

  # Allow manual trigger from GitHub Actions UI
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Get pnpm store directory
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ env.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ROLE_KEY: ${{ secrets.SUPABASE_ROLE_KEY }}
          ARTICLE_TABLE: ${{ secrets.ARTICLE_TABLE }}
          SITE_TABLE: ${{ secrets.SITE_TABLE }}
          CATEGORY_TABLE: ${{ secrets.CATEGORY_TABLE }}
          SUPER_CATEGORY_TABLE: ${{ secrets.SUPER_CATEGORY_TABLE }}
          BOOKMARK_TABLE: ${{ secrets.BOOKMARK_TABLE }}
          ALLOW_HOST_TABLE: ${{ secrets.ALLOW_HOST_TABLE }}
          GENERAL_REMOVE_TAGS_TABLE: ${{ secrets.GENERAL_REMOVE_TAGS_TABLE }}
          GET_SITES_TO_SCRAPE_RPC: ${{ secrets.GET_SITES_TO_SCRAPE_RPC }}
          MAX_ARTICLES: ${{ secrets.MAX_ARTICLES }}
          BATCH_SIZE: ${{ secrets.BATCH_SIZE }}
          SCRAPE_CONCURRENCY: ${{ secrets.SCRAPE_CONCURRENCY || 5 }}
          NODE_ENV: production
        run: pnpm start
